---
date: 2024-04-02T15:43
tags: []
---
Entropy is a *measure* of disorder or **impurity in the given dataset**.
- For a dataset that has n classes and the probability of randomly choosing data from class, i is Pi. Then entropy E(S) can be mathematically represented as

>[!important] formula
>$$E(S) = -\sum_{i=1}^{n} P_i \log_2(P_i)$$


>[!example] 
>If we have a dataset of 10 observations belonging to two classes YES and NO. If 6 observations belong to the class, YES, and 4 observations belong to class NO, then entropy can be written as below.
$$E(S) = -(P_{\text{yes}} \log_2(P_{\text{yes}}) + P_{\text{no}} \log_2(P_{\text{no}}))$$
>$P_{yes}$ is the probability of choosing Yes and $P_{no}$ is the probability of choosing a No. Here $P_{yes}$ is 6/10 and $P_{no}$ is 4/10.
>$$E(S) = -(\frac{6}{10} \cdot \log_2{\frac{6}{10}} +\frac{4}{10} \cdot \log_2{\frac{4}{10}}) \approx 0.971$$
>
>- If all the 10 observations belong to 1 class then entropy will be equal to zero. Which implies the node is a pure node.
>$$E(S) = -(1 \cdot \log_2{1}) = 0$$
>
>- If both classes YES and NO have an equal number of observations, then entropy will be equal to 1.
>$$E(S) = -(\frac{5}{10} \cdot \log_2{\frac{5}{10}} +\frac{5}{10} \cdot \log_2{\frac{5}{10}}) = -2(0.5 \cdot \log_2{0.5}) = 1$$
