Confusion Matrix is a **tabular visualization** of the **ground-truth labels versus model predictions**.
- Each column of the confusion matrix represents the instances in a predicted class 
- each row represents the instances in an actual class.
 
 >[!note] 
 >*Confusion Matrix is not exactly a performance metric but sort of a basis on which other metrics evaluate the results.*
 
 *Each cell in the confusion matrix represents an evaluation factor.*
1. **True Positive (TP)**
the prediction outcome is true, and it is true in reality, also.
2. **True Negative (TN)** 
 the prediction outcome is false, and it is false in reality, also.
3. **False Positive (FP)** 
 prediction outcomes are true, but they are false in actuality.
4. **False Negative (FN)**
predictions are false, and they are true in actuality.

![[Confusion Matrix PMCML 2024-05-21 15.10.13.excalidraw]]
#### [[example of confusion matrix pmcml|example]]
